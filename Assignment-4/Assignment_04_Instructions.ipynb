{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmwWs4S9cRIn"
   },
   "source": [
    "# ECE 57000 Assignment 4 Instructions\n",
    "\n",
    "## **Instructions**\n",
    "This Jupyter notebook document entitled **Assignment_04_Instructions** contains instructions for doing your assignment exercise.\n",
    "A second Jupyter notebook document entited **Assignment_04_Exercise** contains all the exercises that you will need to perform.\n",
    "\n",
    "As you read each section of this instruction, you should try running the associated code snippets. \n",
    "The colaboratory environment allows you to run code snippets locally by clicking on the arrow on the left of the code. This is a wonderful feature that allows you to experiment as you read. You should take advantage of this and experiment and test different ideas, so you can become more familiar with the Python and the Jupyter programing environment. \n",
    "\n",
    "At the end of each sub-section, there will be exercises to perform. \n",
    "You should perform the exercises in the document **Assignment_04_Exercise**, which will contain all your results. \n",
    "You can then hand in your results by printing the **Assignment_04_Exercise** document as a pdf with all code and simulation results included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz2T0QYYpwVR"
   },
   "source": [
    "## Section 1: Introduction to Pytorch tensor\n",
    "\n",
    "In this assignment, we will try to **build a classifier by using neural network**.  \n",
    "Python offers a lot of packages for machine learning, such as Keras, Tensorflow, Pytorch and etc. In this course, we will focus on Pytorch. Pytorch is a popular ML library in Python and is implemented in C and wrapped with Lua. It is developed by Facebook, but now it is widely used in companies such as Twitter, Salesforce.  \n",
    "  \n",
    "One of Pytorch's greatest feature is that it offers **Tensor Computation**. It works just like **Numpy**, but has faster computation and allows for GPU acceleration. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "4Le8d8AyCn0v",
    "outputId": "8d6df0c5-07d3-4629-fad4-1b48ce67e181"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f'-----------Tensor Initialization-----------')\n",
    "# Tensor initilization\n",
    "A = torch.zeros(2,2)\n",
    "print(f'Zero initialization for A: \\nA={A}\\n')\n",
    "\n",
    "A = torch.randn(2,2)\n",
    "print(f'normal distributrion initialization for A: \\nA={A}\\n')\n",
    "\n",
    "print(f'-----------   Tensor Addition    -----------')\n",
    "# Tensor addition\n",
    "A, B = [1,2,3], [3,2,1]\n",
    "A, B = torch.tensor(A), torch.tensor(B)\n",
    "print(f'Tensor A is {A}, Tensor B is {B}')\n",
    "print(f'Tensor addition: \\nA+B={A+B}\\n')\n",
    "\n",
    "print(f'-----------Tensor Initialization-----------')\n",
    "# Tensor indexing and slicing\n",
    "A = torch.ones(3,3)\n",
    "print(f'A is defined as \\n{A}\\n')\n",
    "print(f'The first element :\\n{A[0,0]}\\n')\n",
    "print(f'The first two columns :\\n{A[:,0:2]}\\n')\n",
    "\n",
    "print(f'-----------  Tensor Information-----------')\n",
    "# Tensor information\n",
    "A = torch.rand(3,3)\n",
    "print(f'A has shape: \\n{A.size()}\\n')\n",
    "print(f'A has datatype: \\n{A.dtype}\\n')\n",
    "print(f'A is stored as: \\n{A.type()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pzF08tNGvO5"
   },
   "source": [
    "For more information, please refer to the Pytorch official tutorial : [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-9FIQqwHJoQ"
   },
   "source": [
    "## Section 2: Getting datasets from torchvision\n",
    "Instead of uploading/creating datasets on your own, torchvision offers some popular datasets which is available for download only by writing a few lines of code. The avaliable datasets are MNIST, FMNIST, LSUN, CIFAR, etc. More information on the dataset is available here: [Torchvision dataset](https://pytorch.org/vision/stable/datasets.html)\n",
    "In this assignment, we will use the MNIST dataset. MNIST is a large dataset of handwritten digits. The dataset contains 60,000 train images and 10,000 testing images. Each image is in gray scale and has the size 28x28.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHFbb4BSPkgN"
   },
   "source": [
    "There are several parameters when you trying to get MNIST data from torchvision by using the function `torchvision.dataset.MNIST()`:\n",
    "\n",
    "\n",
    "*   train : This parameter indicates whether you want the training set or the testing set\n",
    "*   download: Set True to start download from the website\n",
    "*   transform: pre-processing functions for the dataset\n",
    "\n",
    "Here is a typical setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "3H7BAJ2LOcbt",
    "outputId": "59d000ae-8114-4ea9-b770-fa2d777be8ba"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\"\"\"\n",
    "Here the transform is a pipeline containing two seperate transforms: \n",
    "1. Transform the data into tensor type\n",
    "2. Normalize the dataset by a giving mean and std. \n",
    "  (Those number is given as the global mean and standard deviation of MNIST dataset)\n",
    "\"\"\"\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                      torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('/data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST('/data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymMofBB4UHSn"
   },
   "source": [
    "Then we need to set up a **getter** for the dataset by using the function `torch.utils.data.DataLoader()`, some parameters is given as:\n",
    "\n",
    "*   batch_size: how many datasets you want each time\n",
    "*   shuffle: whether the extracted data are shuffled from the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KyPJGQ33UE10",
    "outputId": "858536f2-af10-44d0-e9b1-1f958ed0190d"
   },
   "outputs": [],
   "source": [
    "batch_size_train, batch_size_test = 64, 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esNcpoAY56sf"
   },
   "source": [
    "Here the train_loader/test_loader is an iterable, we can extract by using the python built-in function `next()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "Sbjtx4NC6JWu",
    "outputId": "82634ff5-086c-434d-ded9-42d1eecd617a"
   },
   "outputs": [],
   "source": [
    "batch_idx, (images, targets) = next(enumerate(train_loader))\n",
    "print(f'current batch index is {batch_idx}')\n",
    "print(f'images has shape {images.size()}')\n",
    "print(f'targets has shape {targets.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUfdUGjHBCoa"
   },
   "source": [
    "**Important Note:** In PyTorch, image files are stored in the format of (Batchsize x Channel x Height x Width)  \n",
    "We can visualize the first few images and its associated labels like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "7MhwzQ05Bhw5",
    "outputId": "4f0aa5d7-d83a-40c4-824f-4d0cc954aeed"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3,3)\n",
    "fig.set_size_inches(12,12)\n",
    "for i in range(3):\n",
    "  for j in range(3):\n",
    "    ax[i,j].imshow(images[i*3+j][0], cmap='gray')\n",
    "    ax[i,j].set_title(f'label {targets[i*3+j]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kqHb9SYEaoM"
   },
   "source": [
    "## Section 3: Building the neural network structure\n",
    "\n",
    "In the instructions, we will build a simple neural network which utilize the following layers:\n",
    "\n",
    "\n",
    "*   fully connected layers: `nn.Linear(input_dim, output_dim)`\n",
    "*   convolution layers: `nn.Conv2d(input_channel, output_channel, kernel_size)`\n",
    "*   Relu function: `F.relu(input_)`\n",
    "*   max pooling: `F.max_pool2d(input_, kernal_size)`\n",
    "*   log softmax: `F.log_soft_max(input_)`\n",
    "\n",
    "More details on this functions are listed here:\n",
    "[torch.nn.*](https://pytorch.org/docs/stable/nn.html)\n",
    "[torch.nn.functional.*](https://pytorch.org/docs/stable/nn.functional.html)\n",
    "\n",
    "Here is how a standard neural network setup:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfJCw4WDJge6"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OurNN(nn.Module): # Any neural generated network should be generate\n",
    "\n",
    "  def __init__(self):\n",
    "    super(OurNN, self).__init__()\n",
    "\n",
    "    self.conv = nn.Conv2d(1, 3, kernel_size=5)\n",
    "    self.fc = nn.Linear(432, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)        # x now has shape (batchsize x 3 x 24 x 24)\n",
    "    x = F.relu(F.max_pool2d(x,2))  # x now has shape (batchsize x 3 x 12 x 12)\n",
    "    x = x.view(-1, 432)      # x now has shape (batchsize x 432)\n",
    "    x = F.relu(self.fc(x))     # x has shape (batchsize x 10)\n",
    "    return F.log_softmax(x,-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nkhatIfMR0i"
   },
   "source": [
    "Note: Always keep track of the dimension of the **x** throughout the neural network. The dimension can easily get mis-mismatched due to the parameter setup for various layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKQfojVmNPSg"
   },
   "source": [
    "We further need to set up an optimizer to help us backprop the network and learn all its parameters. We use the stochastic gradient descent optimizer: `optim.SGD(model, lr, momentum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlZ7KHWfNr6w"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "classifier = OurNN()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I0aWE__PIdp"
   },
   "source": [
    "## Section 4: Training/Test our neural network\n",
    "\n",
    "Generally we need a training function `train()` that completes the following tasks:\n",
    "\n",
    "1.   init our optimizer\n",
    "2.   get batches of data\n",
    "3.   feed forward the data into our network\n",
    "4.   compute the loss between the output of our network and actual label\n",
    "5.   move a step on the gradient by optimizer\n",
    "6.   output some visual information on what we do\n",
    "\n",
    "Also for the test function `test()`, we have the following tasks:\n",
    "\n",
    "1.   get batches of data\n",
    "2.   feed forward the data into our network\n",
    "3.   compute the loss between the output of our network and actual label\n",
    "4.   calculate our correctness of the output\n",
    "5.   save and output some inforamtion on what we do\n",
    "\n",
    "Here is the code for how we might implement the ideas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeTXly0bSuMA"
   },
   "outputs": [],
   "source": [
    "def train(classifier, epoch):\n",
    "\n",
    "  classifier.train() # we need to set the mode for our model\n",
    "\n",
    "  for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = classifier(images)\n",
    "    loss = F.nll_loss(output, targets) # Here is a typical loss function (negative log likelihood)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 10 == 0: # We record our output every 10 batches\n",
    "      train_losses.append(loss.item()) # item() is to get the value of the tensor directly\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "    if batch_idx % 100 == 0: # We visulize our output every 10 batches\n",
    "      print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "def test(classifier, epoch):\n",
    "\n",
    "  classifier.eval() # we need to set the mode for our model\n",
    "\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "      output = classifier(images)\n",
    "      test_loss += F.nll_loss(output, targets, reduction='sum').item()\n",
    "      pred = output.data.max(1, keepdim=True)[1] # we get the estimate of our result by look at the largest class value\n",
    "      correct += pred.eq(targets.data.view_as(pred)).sum() # sum up the corrected samples\n",
    "  \n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_counter.append(len(train_loader.dataset)*epoch)\n",
    "\n",
    "  print(f'Test result on epoch {epoch}: Avg loss is {test_loss}, Accuracy: {100.*correct/len(test_loader.dataset)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "qz2lODiwV1cd",
    "outputId": "a1cbfd65-9c35-4f64-8f0a-8bb54e74f65b"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = []\n",
    "max_epoch = 3\n",
    "\n",
    "for epoch in range(1, max_epoch+1):\n",
    "  train(classifier, epoch)\n",
    "  test(classifier, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVpFqGHGd13Q"
   },
   "source": [
    "This simple neural network already achieves an overall accuracy of 87.77%. (Note: random guesses would have an accuracy of 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVjKcyOfa2x-"
   },
   "source": [
    "## Section 5: Visualiaze our result\n",
    "Here we plot our loss function graph and some of our predictions:  \n",
    "Loss function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "qBDVbb7ibWfE",
    "outputId": "8974a5fb-7136-4c9d-fc46-e67425d49295"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExYV_yvadQE6"
   },
   "source": [
    "Judging from our loss graph, our network actually converges at only 1 epoch.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_04_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
